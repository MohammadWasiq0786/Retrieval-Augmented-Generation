{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q euriai[langgraph]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVVU7p-QFdGB",
        "outputId": "9eb4bea8-1d0a-433b-ae48-f6224b94cc0c"
      },
      "id": "NVVU7p-QFdGB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "af419e26",
      "metadata": {
        "id": "af419e26"
      },
      "outputs": [],
      "source": [
        "from euriai.langgraph import EuriaiLangGraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EURI_API_KEY = \"euri-.....\"\n",
        "FILE_NAME= \"rag_contenxt.txt\""
      ],
      "metadata": {
        "id": "n6egsWkiFniD"
      },
      "id": "n6egsWkiFniD",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "39fa005f",
      "metadata": {
        "id": "39fa005f"
      },
      "outputs": [],
      "source": [
        "graph = EuriaiLangGraph(\n",
        "    api_key= EURI_API_KEY,\n",
        "    default_model= \"gpt-4.1-nano\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12b7bb59",
      "metadata": {
        "id": "12b7bb59"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "def reseracher_node_fun(state):\n",
        "    query  = state[\"input\"]\n",
        "    file_path = FILE_NAME\n",
        "    context = Path(file_path).read_text()\n",
        "\n",
        "    return {\n",
        "        \"query\":query,\n",
        "        \"context\": context[:3000]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "27f899a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27f899a4",
        "outputId": "dfb480b5-52b9-4552-e075-e3549e54838d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added function node: extractoin_and_query\n"
          ]
        }
      ],
      "source": [
        "graph.add_function_node(\"extractoin_and_query\",reseracher_node_fun)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cf6d8fa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf6d8fa9",
        "outputId": "d1d7d99e-84d8-4289-da71-7e96c6acf474"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nodes': ['extractoin_and_query'],\n",
              " 'ai_nodes': [],\n",
              " 'edges': [],\n",
              " 'conditional_edges': [],\n",
              " 'entry_point': None,\n",
              " 'finish_point': None}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "graph.get_graph_structure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "66abc59c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66abc59c",
        "outputId": "151a267a-f5d1-4beb-bb9c-f7493ff20218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added AI node: synthesizer (model: gpt-4.1-nano)\n"
          ]
        }
      ],
      "source": [
        "graph.add_ai_node(\n",
        "    \"synthesizer\",\n",
        "    prompt_template=\"summarize the following document input that i hvae give to you in veyr very detailed:\\n {context}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f030edcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f030edcb",
        "outputId": "64ed488b-5137-4ea5-f6a6-8e399e39808a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nodes': ['extractoin_and_query', 'synthesizer'],\n",
              " 'ai_nodes': ['synthesizer'],\n",
              " 'edges': [],\n",
              " 'conditional_edges': [],\n",
              " 'entry_point': None,\n",
              " 'finish_point': None}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "graph.get_graph_structure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8faa765f",
      "metadata": {
        "id": "8faa765f"
      },
      "outputs": [],
      "source": [
        "def classify_topic(state):\n",
        "    text = state[\"context\"].lower()\n",
        "    if \"llm\" in text:\n",
        "        topic = \"LLM Workflows\"\n",
        "    elif \"rag\" in text:\n",
        "        topic = \"RAG Pipelines\"\n",
        "    elif \"langchain\" in text:\n",
        "        topic = \"LangChain Integration\"\n",
        "    else:\n",
        "        topic = \"General AI\"\n",
        "\n",
        "    return {**state, \"topic\": topic}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "832da165",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "832da165",
        "outputId": "26bc9abc-276c-483a-8e13-eda1d179e3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added function node: classifier\n"
          ]
        }
      ],
      "source": [
        "graph.add_function_node('classifier',classify_topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c46335f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c46335f1",
        "outputId": "d9bd91d7-a22a-4ce7-c738-9f81bdaf3fbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nodes': ['extractoin_and_query', 'synthesizer', 'classifier'],\n",
              " 'ai_nodes': ['synthesizer'],\n",
              " 'edges': [],\n",
              " 'conditional_edges': [],\n",
              " 'entry_point': None,\n",
              " 'finish_point': None}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "graph.get_graph_structure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4962c84f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4962c84f",
        "outputId": "393de1ae-2d37-4e0d-f433-c85af155e7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added AI node: finalizer (model: gpt-4.1-nano)\n"
          ]
        }
      ],
      "source": [
        "graph.add_ai_node(\n",
        "    \"finalizer\",\n",
        "    prompt_template={\n",
        "        \"User Query: {query}\\n\\nTopic: {topic}\\n\\nContext Summary: {context}\\n\\n\"\n",
        "        \"Answer the query clearly using all context above:\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f7b22f7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7b22f7d",
        "outputId": "ba3a8a8f-6d2b-494b-db11-1e5410416125"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nodes': ['extractoin_and_query', 'synthesizer', 'classifier', 'finalizer'],\n",
              " 'ai_nodes': ['synthesizer', 'finalizer'],\n",
              " 'edges': [],\n",
              " 'conditional_edges': [],\n",
              " 'entry_point': None,\n",
              " 'finish_point': None}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "graph.get_graph_structure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "18382241",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18382241",
        "outputId": "5142b2c3-8814-4cc8-a87a-77d1ef5840cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set entry point: extractoin_and_query\n",
            "Added edge: extractoin_and_query -> synthesizer\n"
          ]
        }
      ],
      "source": [
        "graph.set_entry_point('extractoin_and_query')\n",
        "graph.add_edge('extractoin_and_query',\"synthesizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9ea0a4c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ea0a4c6",
        "outputId": "73ceb030-9d55-4095-c28c-7ad166e0f4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added edge: synthesizer -> classifier\n"
          ]
        }
      ],
      "source": [
        "graph.add_edge(\"synthesizer\",'classifier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "38d68b43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38d68b43",
        "outputId": "8f95b6b8-ce65-455a-93a0-260709250214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added edge: classifier -> finalizer\n"
          ]
        }
      ],
      "source": [
        "graph.add_edge('classifier',\"finalizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f69f3eaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f69f3eaa",
        "outputId": "44568eae-d8b8-4f69-bb96-80301f41d9ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set finish point: finalizer\n"
          ]
        }
      ],
      "source": [
        "graph.set_finish_point(\"finalizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3ae80557",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ae80557",
        "outputId": "73a950c1-88ad-4c01-aa30-c3818dde2136"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nodes': ['extractoin_and_query', 'synthesizer', 'classifier', 'finalizer'],\n",
              " 'ai_nodes': ['synthesizer', 'finalizer'],\n",
              " 'edges': [('extractoin_and_query', 'synthesizer'),\n",
              "  ('synthesizer', 'classifier'),\n",
              "  ('classifier', 'finalizer')],\n",
              " 'conditional_edges': [],\n",
              " 'entry_point': 'extractoin_and_query',\n",
              " 'finish_point': 'finalizer'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "graph.get_graph_structure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f13babbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f13babbf",
        "outputId": "62ce7205-b250-4788-dd41-69ad9ff18afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph compiled successfully\n",
            "\u001b[1m[values]\u001b[0m {'input': 'how can langgraph and euri be used to build a multi agent rag systme'}\n",
            "\u001b[1m[updates]\u001b[0m {'extractoin_and_query': {'query': 'how can langgraph and euri be used to build a multi agent rag systme', 'context': 'LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\\n\\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\\n'}}\n",
            "\u001b[1m[values]\u001b[0m {'query': 'how can langgraph and euri be used to build a multi agent rag systme', 'context': 'LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\\n\\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\\n'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error in AI node finalizer after 1 attempts: 'set' object has no attribute 'format'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m[updates]\u001b[0m {'synthesizer': {'query': 'how can langgraph and euri be used to build a multi agent rag systme', 'context': 'LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\\n\\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\\n', 'synthesizer_output': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'synthesizer_raw_response': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\"}}\n",
            "\u001b[1m[values]\u001b[0m {'query': 'how can langgraph and euri be used to build a multi agent rag systme', 'context': 'LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\\n\\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\\n', 'synthesizer_output': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'synthesizer_raw_response': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\"}\n",
            "\u001b[1m[updates]\u001b[0m {'classifier': {'query': 'how can langgraph and euri be used to build a multi agent rag systme', 'context': 'LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\\n\\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\\n', 'synthesizer_output': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'synthesizer_raw_response': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'topic': 'LLM Workflows'}}\n",
            "\u001b[1m[values]\u001b[0m {'query': 'how can langgraph and euri be used to build a multi agent rag systme', 'context': 'LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\\n\\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\\n', 'synthesizer_output': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'synthesizer_raw_response': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'topic': 'LLM Workflows'}\n",
            "\u001b[1m[updates]\u001b[0m {'finalizer': {'query': 'how can langgraph and euri be used to build a multi agent rag systme', 'context': 'LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\\n\\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\\n', 'synthesizer_output': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'synthesizer_raw_response': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'topic': 'LLM Workflows', 'finalizer_error': \"'set' object has no attribute 'format'\", 'error_handled': False}}\n",
            "\u001b[1m[values]\u001b[0m {'query': 'how can langgraph and euri be used to build a multi agent rag systme', 'context': 'LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\\n\\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\\n', 'synthesizer_output': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'synthesizer_raw_response': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'topic': 'LLM Workflows', 'finalizer_error': \"'set' object has no attribute 'format'\", 'error_handled': False}\n",
            "Workflow completed in 7.56s\n",
            "final output:\n",
            " {'query': 'how can langgraph and euri be used to build a multi agent rag systme', 'context': 'LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\\n\\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\\n', 'synthesizer_output': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'synthesizer_raw_response': \"Certainly! Here's a very detailed summary of the provided document:\\n\\nLangGraph Overview:\\n- LangGraph is an open-source framework developed in Python designed to facilitate the construction of complex workflows involving large language models (LLMs). \\n- Its architecture emphasizes statefulness, composability, and observability, enabling users to create modular and transparent LLM applications.\\nCore Components and Primitives:\\n- Nodes: Fundamental units within LangGraph that represent computational steps or processes, such as invoking LLMs, processing data, or executing specific functions.\\n- Memory: Mechanisms for maintaining and managing persistent or contextual information across different nodes and stages in the workflow, allowing for context-aware operations.\\n- Edges: Connections that define data flow and dependencies between nodes, establishing the sequence and interaction within the workflow.\\n- Agents: Entities that coordinate multiple nodes or components, possibly managing decision-making, task orchestration, or multi-agent interactions.\\nIntegration Capabilities:\\n- LangChain: LangGraph supports integration with LangChain, a popular framework for building LLM applications, enabling seamless chaining and management of language models.\\n- LlamaIndex: Compatibility with LlamaIndex (formerly GPT Index) allows for efficient indexing and retrieval of large document collections, enhancing retrieval-augmented generation workflows.\\n- Vector Stores and Retrievers: Supports various vector databases and retrieval mechanisms to efficiently fetch relevant information, improving the quality and relevance of generated outputs.\\nWorkflow Features:\\n- Multi-Agent Pipelines: Users can construct pipelines involving multiple agents working collaboratively or sequentially, each maintaining its own memory and utilizing retrievers, tools, or external APIs.\\n- Memory and State Management: The framework enables workflows that retain context over time, facilitating complex interactions and sustained conversations.\\n- Retrievers and Tools: Incorporation of retrieval mechanisms and auxiliary tools to augment LLM capabilities, such as accessing external knowledge bases or performing specialized computations.\\n- Streaming Outputs: Supports real-time streaming of generated outputs, allowing for interactive and dynamic user experiences.\\nIntegration with EURI AI:\\n- EURI AI enhances LangGraph by powering each node with real-time inference capabilities, utilizing models like gpt-4.1-nano.\\n- This integration ensures that each component in the workflow can perform low-latency, high-efficiency inference, enabling real-time applications and responsive interactions.\\nIn essence, LangGraph provides a comprehensive, flexible, and extensible framework for building sophisticated LLM workflows, leveraging rich integrations, state management, and real-time inference to support advanced AI applications.\", 'topic': 'LLM Workflows', 'finalizer_error': \"'set' object has no attribute 'format'\", 'error_handled': False}\n"
          ]
        }
      ],
      "source": [
        "result = graph.run({\"input\":\"how can langgraph and euri be used to build a multi agent rag systme\"})\n",
        "print(\"final output:\\n\" ,result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c0f48f9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "c0f48f9b",
        "outputId": "64bb9be9-6e2f-4a37-c211-38f7c7eb37df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "how can langgraph and euri be used to build a multi agent rag systme"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "LangGraph is an open-source Python framework for building stateful, composable, and observable LLM workflows. It provides primitives for defining nodes, memory, edges, and agents. LangGraph supports integration with LangChain, LlamaIndex, and various vector stores and retrievers.\n\nYou can build multi-agent pipelines with memory, retrievers, tools, and streaming outputs. EURI AI integrates by powering each node with real-time inference using models like gpt-4.1-nano.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown, Latex\n",
        "display(Markdown(result['query']))\n",
        "display(Markdown(result['context']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBzLlHmtGQNF"
      },
      "id": "HBzLlHmtGQNF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lang_graph",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}