# ğŸ“š Documnet Loading Assignment

- ğŸ“„ Loading `.txt`, `.csv`, `.docx`, Notion, and URL content
- âœ‚ï¸ Chunking with Recursive, Token, and Sentence strategies
- ğŸ§¾ Metadata injection and filtering
- ğŸ”¬ Token vs Word chunk size analysis
- ğŸ“¡ Generating embeddings using EURI and HuggingFace
- ğŸ” Semantic search using FAISS
- ğŸ“ˆ Visualizations and similarity heatmaps

---

## ğŸ“ Project Structure

```bash
Assignment/
â”œâ”€â”€ generate_test_data.py      # Generate long, realistic test data
â”œâ”€â”€ loaders.py                 # Load various document types
â”œâ”€â”€ chunking.py                # Recursive, token-based, sentence chunking
â”œâ”€â”€ metadata_filter.py         # Add metadata and apply chunk filters
â”œâ”€â”€ text_splitters.py          # Built-in and custom sentence splitters
â”œâ”€â”€ chunk_analysis.py          # Chunk size vs count analysis + charts
â”œâ”€â”€ embeddings.py              # EURI & HuggingFace embeddings + FAISS search
â”œâ”€â”€ main.py                    # Run everything end-to-end
â”œâ”€â”€ README.md                  # You're here
â””â”€â”€ test_data/                 # Auto-generated input/output files
````

---

## ğŸš€ Run the Full Pipeline

```bash
python main.py
```
---

### Chunk Size Comparison

![chunkSize](./test_data/chunk_size_comparison.png)

### Embedding Similarity Comparison

![embedding](./test_data/embedding_similarity_comparison.png)

---
